{
  
    
        "post0": {
            "title": "Options for finding neighbors",
            "content": "Yesterday, we found that some samples are still far from others after KNN correction. Why do we see this pattern? In this notebook, I have explored different options of calculating neighbors. . Important: I modified the KNN function. Earlier, the distance matrix was always positive and sorting the samples by distance had the self index at the beginning (distance = 0). . Setup . This notebook requires: . LD filtered genotype | Gene expression matrix | Data location . #collapse-hide dosagefile = &#39;/cbscratch/sbanerj/gtex_pca/gtex_v8_filtered.dosage.raw&#39; dosage_numpy_file = &#39;/cbscratch/sbanerj/gtex_pca/gtex_dosage.npy&#39; expression_file = &#39;/scratch/sbanerj/trans-eqtl/input/gtex_v8/expression/gtex_ms_raw_std_protein_coding_lncRNA.txt&#39; . . Python libraries . #collapse-hide import numpy as np import pandas as pd from sklearn.decomposition import PCA from scipy import stats import os from scipy.cluster import hierarchy as hc import matplotlib.pyplot as plt import matplotlib from mpl_toolkits.axes_grid1 import make_axes_locatable from utils import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 300) . . Read input data . #collapse-hide def read_gtex(filename): # returns N x G gene expression expr_list = list() donor_list = list() gene_list = list() with open(filename) as mfile: donor_list = mfile.readline().strip().split(&quot; t&quot;)[1:] for line in mfile: linesplit = line.strip().split(&quot; t&quot;) gene = linesplit[0].strip() gene_list.append(gene) expr = np.array([float(x) for x in linesplit[1:]]) expr_list.append(expr) expr = np.transpose(np.array(expr_list)) return expr, donor_list, gene_list def center_expression(Y): &#39;&#39;&#39; Y is N x G here we center the columns, the mean of the columns (genes) are subtracted &#39;&#39;&#39; Ycent = Y - np.mean(Y, axis = 0) return Ycent def center_genotype(X): &#39;&#39;&#39; X is N x I here we center the columns, the mean of the columns (SNPs) are subtracted &#39;&#39;&#39; return X - np.mean(X, axis = 0).reshape(1, -1) if not os.path.isfile(dosage_numpy_file): dosage = np.loadtxt(dosagefile, delimiter=&#39; &#39;, skiprows=1, usecols=range(6, 97612)) np.save(dosage_numpy_file, dosage) else: dosage = np.load(dosage_numpy_file) gtsamples = list() with open (dosagefile, &#39;r&#39;) as infile: next(infile) for line in infile: gtsamples.append(line.strip().split()[1]) gx, gxsamples, _ = read_gtex(expression_file) gx = center_expression(gx) sampleidx = [gtsamples.index(x) for x in gxsamples] # assumes all expression samples have genotype dreduce = dosage[sampleidx, :] gt = center_genotype(dreduce) #dreduce - np.mean(dreduce, axis = 0).reshape(1, -1) print(f&#39;{len(sampleidx)} samples, {gx.shape[1]} genes, {gt.shape[1]} SNPs.&#39;) print(f&#39;Centered and normalized genotype and expression. Samples in same order as `gxsamples`&#39;) . . 706 samples, 13236 genes, 97606 SNPs. Centered and normalized genotype and expression. Samples in same order as `gxsamples` . Core function modules . Calculate PCA, distance matrix and KNN. | Map distance matrix from one sample space to another. | Remove first N principal components from any matrix X. | Plotting function. | #collapse-hide def get_pca(x, K): pca = PCA(n_components=K) pca.fit(x) # requires N x P (n_samples, n_features) x_pca = pca.transform(x) return x_pca def get_distance(a, b): return np.linalg.norm(a - b) def distance_matrix(x_pca): nsample = x_pca.shape[0] distance_matrix = np.zeros((nsample, nsample)) for i in range(nsample): for j in range(i+1, nsample): dist = get_distance(x_pca[i,:], x_pca[j,:]) distance_matrix[i, j] = dist distance_matrix[j, i] = dist return distance_matrix def map_distance_matrix(dm, samples, target_samples): N = len(target_samples) newdm = np.zeros((N, N)) newdm[:] = np.nan for i in range(N): if target_samples[i] in samples: newdm[i, i] = 0 # diagonal is always zero iold = samples.index(target_samples[i]) for j in range(i+1, N): if target_samples[j] in samples: jold = samples.index(target_samples[j]) newdm[i, j] = dm[iold, jold] newdm[j, i] = dm[jold, iold] return newdm def knn(gx, gt, dm, K, center = True): assert (gx.shape[0] == gt.shape[0]) N = gx.shape[0] gx_knn = np.zeros_like(gx) gt_knn = np.zeros_like(gt) for i in range(N): #neighbors = np.argsort(distance_matrix[i, :kneighbor + 1]) ordered_nbrs = np.argsort(dm[i, :]) self_idx = np.where(ordered_nbrs == i)[0][0] neighbors = np.delete(ordered_nbrs, self_idx)[:K] gx_knn[i, :] = gx[i, :] - np.mean(gx[neighbors, :], axis = 0) gt_knn[:, i] = gt[:, i] - np.mean(gt[:, neighbors], axis = 1) if center: gx_knn -= np.mean(gx_knn, axis = 0) gt_knn -= np.mean(gt_knn, axis = 0) return gx_knn, gt_knn def remove_nfirst_pcs(X, n=1): Xnorm = X U, S, Vt = np.linalg.svd(X, full_matrices=False) Xhat = U[:, n:] @ np.diag(S[n:]) @ Vt[n:, :] return Xhat def center_scale_off_diagonal(X): ioff = np.where(~np.eye(X.shape[0], dtype=bool)) X[ioff] = (X[ioff] - np.mean(X[ioff])) / np.std(X[ioff]) return X def plot_distance_matrices(dmA, dmB, norms = None): &#39;&#39;&#39; provide norms, if required, as norms = (norm1, norm2) where, norm1 = matplotlib.colors.DivergingNorm(vmin=10., vcenter=90., vmax=170.) norm2 = matplotlib.colors.DivergingNorm(vmin=0., vcenter=90., vmax=300.) &#39;&#39;&#39; fig = plt.figure(figsize = (12, 6)) ax1 = fig.add_subplot(121) ax2 = fig.add_subplot(122) # the zero distance between the same samples # is bad for the color scale. dmA[np.diag_indices(dmA.shape[0])] = np.nan dmB[np.diag_indices(dmB.shape[0])] = np.nan cmap1 = plt.get_cmap(&quot;YlOrRd&quot;) cmap1.set_bad(&#39;w&#39;) cmap2 = plt.get_cmap(&quot;YlGnBu&quot;) cmap2.set_bad(&#39;w&#39;) if norms is not None: norm1 = norms[0] norm2 = norms[1] im1 = ax1.imshow(dmA, cmap = cmap1, norm = norm1, interpolation=&#39;nearest&#39;) im2 = ax2.imshow(dmB, cmap = cmap2, norm = norm2, interpolation=&#39;nearest&#39;) else: im1 = ax1.imshow(dmA, cmap = cmap1, interpolation=&#39;nearest&#39;) im2 = ax2.imshow(dmB, cmap = cmap2, interpolation=&#39;nearest&#39;) divider = make_axes_locatable(ax1) cax = divider.append_axes(&quot;right&quot;, size=&quot;5%&quot;, pad=0.2) cbar = plt.colorbar(im1, cax=cax, fraction = 0.1) divider = make_axes_locatable(ax2) cax = divider.append_axes(&quot;right&quot;, size=&quot;5%&quot;, pad=0.2) cbar = plt.colorbar(im2, cax=cax, fraction = 0.1) ax1.set_title(&quot;Genotype space&quot;, pad = 20) ax2.set_title(&quot;Expression space&quot;, pad = 20) plt.tight_layout() return fig . . 1. Find neighbors after removing first PC from distance matrix . Johannes suggested that removing the first PC might give better neighbors. . #collapse-show # Before KNN DT = 20 # reduced dimension of genotype DX = 30 # reduced dimension of expression dm_gt = distance_matrix(get_pca(gt, DT)) dm_gx = distance_matrix(get_pca(gx, DX)) dm_gx_corr = remove_nfirst_pcs(dm_gx, n=1) # Single-KNN K = 30 gx_knn, gt_knn = knn(gx, gt, dm_gx_corr, K) dm_gt_knn = distance_matrix(get_pca(gt_knn, DT)) dm_gx_knn = distance_matrix(get_pca(gx_knn, DX)) . . I ordered the samples by their distance in the expression space . #collapse-hide o2 = hc.leaves_list(hc.linkage(dm_gx_corr, method = &#39;centroid&#39;)) . . Plot the distance matrices . #collapse-hide mgt = dm_gt[o2, :][:, o2] mgx = dm_gx_corr[o2, :][:, o2] norm1 = matplotlib.colors.DivergingNorm(vmin=10., vcenter=90., vmax=170.) norm2 = matplotlib.colors.DivergingNorm(vmin=-50, vcenter=40, vmax=100.) norms = (norm1, norm2) fig = plot_distance_matrices(mgt, mgx, norms = norms) fig.suptitle(&quot;Modified distance matrix before KNN&quot;) plt.show() . . #collapse-hide mgt = dm_gt_knn[o2, :][:, o2] mgx = dm_gx_knn[o2, :][:, o2] norm1 = matplotlib.colors.DivergingNorm(vmin=10., vcenter=90., vmax=170.) norm2 = matplotlib.colors.DivergingNorm(vmin=0., vcenter=75., vmax=300.) norms = (norm1, norm2) fig = plot_distance_matrices(mgt, mgx, norms = norms) fig.suptitle(&quot;Original distance matrix after KNN&quot;) plt.show() . . 2. Find neighbors after removing first PC from log(distance matrix) . Second suggestion of Johannes to get better neighbors . #collapse-show # Before KNN DT = 20 # reduced dimension of genotype DX = 30 # reduced dimension of expression dm_gt = distance_matrix(get_pca(gt, DT)) dm_gx = distance_matrix(get_pca(gx, DX)) dm_gx_log = np.log(dm_gx + 1.) dm_gx_log_corr = remove_nfirst_pcs(dm_gx_log, n=1) dm_gx_corr = np.exp(dm_gx_log_corr) - 1. # Single-KNN K = 30 gx_knn, gt_knn = knn(gx, gt, dm_gx_corr, K) dm_gt_knn = distance_matrix(get_pca(gt_knn, DT)) dm_gx_knn = distance_matrix(get_pca(gx_knn, DX)) . . I ordered the samples by their distance in the expression space . #collapse-hide o2 = hc.leaves_list(hc.linkage(dm_gx_corr, method = &#39;centroid&#39;)) . . Plot the distance matrices . #collapse-hide mgt = dm_gt[o2, :][:, o2] mgx = dm_gx_corr[o2, :][:, o2] norm1 = matplotlib.colors.DivergingNorm(vmin=10., vcenter=90., vmax=170.) norm2 = matplotlib.colors.DivergingNorm(vmin=-0.75, vcenter=0, vmax=0.75) norms = (norm1, norm2) fig = plot_distance_matrices(mgt, mgx, norms = norms) fig.suptitle(&quot;Modified distance matrix before KNN&quot;) plt.show() . . #collapse-hide mgt = dm_gt_knn[o2, :][:, o2] mgx = dm_gx_knn[o2, :][:, o2] norm1 = matplotlib.colors.DivergingNorm(vmin=10., vcenter=90., vmax=170.) norm2 = matplotlib.colors.DivergingNorm(vmin=0., vcenter=75., vmax=300.) norms = (norm1, norm2) fig = plot_distance_matrices(mgt, mgx, norms = norms) fig.suptitle(&quot;Original distance matrix after KNN&quot;) plt.show() . . 3. Calculate neigbors afer centering / scaling distance matrix . The distance matrix is not centered and scaled. Since the matrix is not centered, the first PC captures the intercept . Therefore, I thought centering / scaling the matrix would give same results as removing the first PC and maybe, even better. . While centering and scaling, the diagonal elements were left at 0. . #collapse-show # Before KNN DT = 20 # reduced dimension of genotype DX = 30 # reduced dimension of expression dm_gt = distance_matrix(get_pca(gt, DT)) dm_gx = distance_matrix(get_pca(gx, DX)) dm_gx_corr = center_scale_off_diagonal(dm_gx) # Single-KNN K = 30 gx_knn, gt_knn = knn(gx, gt, dm_gx_corr, K) dm_gt_knn = distance_matrix(get_pca(gt_knn, DT)) dm_gx_knn = distance_matrix(get_pca(gx_knn, DX)) . . I ordered the samples by their distance in the expression space . #collapse-hide o2 = hc.leaves_list(hc.linkage(dm_gx_corr, method = &#39;centroid&#39;)) . . Plot the distance matrices . #collapse-hide mgt = dm_gt[o2, :][:, o2] mgx = dm_gx_corr[o2, :][:, o2] norm1 = matplotlib.colors.DivergingNorm(vmin=10., vcenter=90., vmax=170.) norm2 = matplotlib.colors.DivergingNorm(vmin=-0.75, vcenter=0, vmax=0.75) norms = (norm1, norm2) fig = plot_distance_matrices(mgt, mgx) fig.suptitle(&quot;Modified distance matrix before KNN&quot;) plt.show() . . #collapse-hide mgt = dm_gt_knn[o2, :][:, o2] mgx = dm_gx_knn[o2, :][:, o2] norm1 = matplotlib.colors.DivergingNorm(vmin=10., vcenter=90., vmax=170.) norm2 = matplotlib.colors.DivergingNorm(vmin=0., vcenter=75., vmax=300.) norms = (norm1, norm2) fig = plot_distance_matrices(mgt, mgx, norms = norms) fig.suptitle(&quot;Original distance matrix after KNN&quot;) plt.show() . . 4. Apply KNN on quantile normalized gene expression . In attempt 3, I found that some samples are really far apart like outliers. I also found that the samples which are dissimilar to others are because of outlying gene expressions. Quantile normalization will force all outliers to have the same normal distribution. . Warning: This is wrong, quantile normalization removes trans-eQTL information! . Quantile normalize the gene expression matrix . #collapse-hide def normalize_quantiles(M): &quot;&quot;&quot; Note: replicates behavior of R function normalize.quantiles from library(&quot;preprocessCore&quot;) Reference: [1] Bolstad et al., Bioinformatics 19(2), pp. 185-193, 2003 Adapted from https://github.com/andrewdyates/quantile_normalize &quot;&quot;&quot; Q = M.argsort(axis=0) m,n = M.shape # compute quantile vector quantiles = np.zeros(m) for i in range(n): quantiles += M[Q[:,i],i] quantiles = quantiles / n for i in range(n): # Get equivalence classes; unique values == 0 dupes = np.zeros(m, dtype=np.int) for j in range(m-1): if M[Q[j,i],i]==M[Q[j+1,i],i]: dupes[j+1] = dupes[j]+1 # Replace column with quantile ranks M[Q[:,i],i] = quantiles # Average together equivalence classes j = m-1 while j &gt;= 0: if dupes[j] == 0: j -= 1 else: idxs = Q[j-dupes[j]:j+1,i] M[idxs,i] = np.median(M[idxs,i]) j -= 1 + dupes[j] assert j == -1 return M def inverse_normal_transform(M): &quot;&quot;&quot; Transform rows to a standard normal distribution. After quantile normalization of samples, standardize expression of each gene &quot;&quot;&quot; R = stats.mstats.rankdata(M, axis=1) # ties are averaged if isinstance(M, pd.DataFrame): Q = pd.DataFrame(stats.norm.ppf(R/(M.shape[1]+1)), index=M.index, columns=M.columns) else: Q = stats.norm.ppf(R/(M.shape[1]+1)) return Q gx_qn = inverse_normal_transform(normalize_quantiles(gx)) . . Calculate KNN and distance matrices. . #collapse-show # Before KNN DT = 20 # reduced dimension of genotype DX = 30 # reduced dimension of expression dm_gt = distance_matrix(get_pca(gt, DT)) dm_gx = distance_matrix(get_pca(gx_qn, DX)) dm_gx_corr = center_scale_off_diagonal(dm_gx) # Single-KNN K = 30 gx_knn, gt_knn = knn(gx_qn, gt, dm_gx_corr, K) dm_gt_knn = distance_matrix(get_pca(gt_knn, DT)) dm_gx_knn = distance_matrix(get_pca(gx_knn, DX)) . . I ordered the samples by their distance in the expression space . #collapse-hide o2 = hc.leaves_list(hc.linkage(dm_gx_corr, method = &#39;centroid&#39;)) . . Plot the distance matrices . #collapse-hide mgt = dm_gt[o2, :][:, o2] mgx = dm_gx_corr[o2, :][:, o2] norm1 = matplotlib.colors.DivergingNorm(vmin=10., vcenter=90., vmax=170.) norm2 = matplotlib.colors.DivergingNorm(vmin=-0.75, vcenter=0, vmax=0.75) norms = (norm1, norm2) fig = plot_distance_matrices(mgt, mgx) fig.suptitle(&quot;Modified distance matrix before KNN&quot;) plt.show() . . #collapse-hide mgt = dm_gt_knn[o2, :][:, o2] mgx = dm_gx_knn[o2, :][:, o2] norm1 = matplotlib.colors.DivergingNorm(vmin=10., vcenter=90., vmax=170.) norm2 = matplotlib.colors.DivergingNorm(vmin=0., vcenter=75., vmax=300.) norms = (norm1, norm2) fig = plot_distance_matrices(mgt, mgx, norms = norms) fig.suptitle(&quot;Original distance matrix after KNN&quot;) plt.show() . . 5. Calculate neighbors from quantile normalized gene expression and apply on raw data . . Tip: And, this works! . #collapse-show # Before KNN DT = 20 # reduced dimension of genotype DX = 30 # reduced dimension of expression dm_gt = distance_matrix(get_pca(gt, DT)) dm_gx = distance_matrix(get_pca(gx_qn, DX)) dm_gx_corr = center_scale_off_diagonal(dm_gx) # Single-KNN K = 30 gx_knn, gt_knn = knn(gx, gt, dm_gx_corr, K) dm_gt_knn = distance_matrix(get_pca(gt_knn, DT)) dm_gx_knn = distance_matrix(get_pca(gx_knn, DX)) . . I ordered the samples by their distance in the expression space . #collapse-hide o2 = hc.leaves_list(hc.linkage(dm_gx_corr, method = &#39;centroid&#39;)) . . Plot the distance matrices . #collapse-hide mgt = dm_gt[o2, :][:, o2] mgx = dm_gx_corr[o2, :][:, o2] norm1 = matplotlib.colors.DivergingNorm(vmin=10., vcenter=90., vmax=170.) norm2 = matplotlib.colors.DivergingNorm(vmin=-0.75, vcenter=0, vmax=0.75) norms = (norm1, norm2) fig = plot_distance_matrices(mgt, mgx) fig.suptitle(&quot;Modified distance matrix before KNN&quot;) plt.show() . . #collapse-hide mgt = dm_gt_knn[o2, :][:, o2] mgx = dm_gx_knn[o2, :][:, o2] norm1 = matplotlib.colors.DivergingNorm(vmin=10., vcenter=90., vmax=170.) norm2 = matplotlib.colors.DivergingNorm(vmin=0., vcenter=75., vmax=300.) norms = (norm1, norm2) fig = plot_distance_matrices(mgt, mgx, norms = norms) fig.suptitle(&quot;Original distance matrix after KNN&quot;) plt.show() . .",
            "url": "https://banskt.github.io/trans-eqtl-ideas/2020/05/20/options-for-finding-neighbors.html",
            "relUrl": "/2020/05/20/options-for-finding-neighbors.html",
            "date": " • May 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Removing background by PCA",
            "content": "Some references . Background removal with robust PCA A nice explanation of PCA . Setup . Expression file path . #collapse-hide expression_file = &#39;/scratch/sbanerj/trans-eqtl/input/gtex_v8/expression/gtex_ms_raw_std_protein_coding_lncRNA.txt&#39; . . Python libraries . #collapse-hide import numpy as np import pandas as pd from sklearn.decomposition import PCA from scipy import stats import os from scipy.cluster import hierarchy as hc import matplotlib.pyplot as plt import matplotlib from mpl_toolkits.axes_grid1 import make_axes_locatable from utils import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 300) . . Define some functions . Read GTEx data | Reduce matrix dimension | Calculate distance matrix | Plot heatmap | #collapse-hide def read_gtex(filename): # returns N x G gene expression expr_list = list() donor_list = list() gene_list = list() with open(filename) as mfile: donor_list = mfile.readline().strip().split(&quot; t&quot;)[1:] for line in mfile: linesplit = line.strip().split(&quot; t&quot;) gene = linesplit[0].strip() gene_list.append(gene) expr = np.array([float(x) for x in linesplit[1:]]) expr_list.append(expr) expr = np.transpose(np.array(expr_list)) return expr, donor_list, gene_list def get_pca(x, K): pca = PCA(n_components=K) pca.fit(x) # requires N x P (n_samples, n_features) x_pca = pca.transform(x) return x_pca def get_distance(a, b): return np.linalg.norm(a - b) def distance_matrix(x_pca): nsample = x_pca.shape[0] distance_matrix = np.zeros((nsample, nsample)) for i in range(nsample): for j in range(i+1, nsample): dist = get_distance(x_pca[i,:], x_pca[j,:]) distance_matrix[i, j] = dist distance_matrix[j, i] = dist return distance_matrix def pheatmap(ax, Xorig, title, norm = None): &#39;&#39;&#39; provide norm, if required where, norm = matplotlib.colors.DivergingNorm(vmin=0., vcenter=90., vmax=300.) &#39;&#39;&#39; # the zero distance between the same samples # is bad for the color scale. X = Xorig.copy() X[np.diag_indices(X.shape[0])] = np.nan cmap = plt.get_cmap(&quot;YlGnBu&quot;) cmap.set_bad(&#39;w&#39;) if norm is not None: im1 = ax.imshow(X, cmap = cmap, norm = norm, interpolation=&#39;nearest&#39;) else: im1 = ax.imshow(X, cmap = cmap, interpolation=&#39;nearest&#39;) divider = make_axes_locatable(ax) cax = divider.append_axes(&quot;right&quot;, size=&quot;5%&quot;, pad=0.2) cbar = plt.colorbar(im1, cax=cax, fraction = 0.1) ax.set_title(title, pad = 20) return . . Read data . #collapse-hide gx, gxsamples, _ = read_gtex(expression_file) . . Calculate distance matrix . #collapse-hide dm_gx = distance_matrix(get_pca(gx, 30)) o1 = hc.leaves_list(hc.linkage(dm_gx, method = &#39;centroid&#39;)) mgx = dm_gx[o1, :][:, o1] . . /usr/users/sbanerj/miniconda3/envs/py36/lib/python3.7/site-packages/ipykernel_launcher.py:3: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix This is separate from the ipykernel package so we can avoid doing imports until . Remove background using different methods . np.linalg.eig returns . w : (M,) array. The eigenvalues, each repeated according to its multiplicity. The eigenvalues are not necessarily ordered. | v : (M, M) array. The normalized (unit “length”) eigenvectors, such that the column v[:,i] is the eigenvector corresponding to the eigenvalue w[i]. | . np.linalg.svd (a, full_matrices = False) returns . u : (M, K) array. Unitary array(s). | s : (K, ) array. Vector(s) with the singular values, within each vector sorted in descending order. | vt : (K, N) array. Unitary array(s). | . #collapse-hide def remove_nfirst_pcs_svd(X, n=1): &#39;&#39;&#39; Using the svd module of numpy.linalg &#39;&#39;&#39; #mu = np.mean(X, axis = 0) #Xnorm = X - mu #U, S, Vt = np.linalg.svd(Xnorm, full_matrices=False) U, S, Vt = np.linalg.svd(X, full_matrices=False) Xhat = U[:, n:] @ np.diag(S[n:]) @ Vt[n:, :] #Xhat += mu return Xhat def remove_nfirst_pcs_eig(X, n=1): &#39;&#39;&#39; Using the eig module of numpy.linalg &#39;&#39;&#39; #mu = np.mean(X, axis = 0) #Xnorm = X - mu #w, v = np.linalg.eig(Xnorm) w, v = np.linalg.eig(X) iord = np.argsort(w)[::-1] w = w[iord] v = v[:, iord] Xhat = v[:, n:] @ np.diag(w[n:]) @ v[:, n:].T #Xhat += mu return Xhat def remove_nfirst_pcs_skl(X, n = 1): &#39;&#39;&#39; Using the PCA module of sklearn &#39;&#39;&#39; nsamples = X.shape[0] nfeatures = X.shape[1] ncomp = min(nsamples, nfeatures) pca = PCA(n_components = ncomp) pca.fit(X) Xpcs = pca.transform(X) Xeig = pca.components_ mu = np.mean(X, axis=0) Xhat = np.dot(Xpcs[:, n:], Xeig[n:, :]) Xhat += mu return Xhat . . Calculate . #collapse-show mgx_n0_eig = remove_nfirst_pcs_eig(mgx, n=0) mgx_n1_eig = remove_nfirst_pcs_eig(mgx, n=1) mgx_n0_svd = remove_nfirst_pcs_svd(mgx, n=0) mgx_n1_svd = remove_nfirst_pcs_svd(mgx, n=1) mgx_n0_skl = remove_nfirst_pcs_skl(mgx, n=0) mgx_n1_skl = remove_nfirst_pcs_skl(mgx, n=1) . . Plot . Using numpy.linalg.eig . norm = matplotlib.colors.DivergingNorm(vmin=0., vcenter=75., vmax=300.) . #collapse-show fig = plt.figure(figsize = (18, 6)) ax1 = fig.add_subplot(131) ax2 = fig.add_subplot(132) ax3 = fig.add_subplot(133) pheatmap(ax1, mgx, &quot;Original&quot;, norm = norm) pheatmap(ax2, mgx_n0_eig, &quot;Reconstruction&quot;, norm = norm) pheatmap(ax3, mgx_n1_eig, &quot;Removed first PC&quot;, norm = norm) fig.suptitle(&quot;Using np.linalg.eig&quot;) plt.tight_layout() plt.show() . . Using numpy.linalg.svd . #collapse-show fig = plt.figure(figsize = (18, 6)) ax1 = fig.add_subplot(131) ax2 = fig.add_subplot(132) ax3 = fig.add_subplot(133) pheatmap(ax1, mgx, &quot;Original&quot;, norm = norm) pheatmap(ax2, mgx_n0_svd, &quot;Reconstruction&quot;, norm = norm) pheatmap(ax3, mgx_n1_svd, &quot;Removed first PC&quot;, norm = norm) plt.tight_layout() plt.show() . . Using sklearn.PCA . #collapse-show fig = plt.figure(figsize = (18, 6)) ax1 = fig.add_subplot(131) ax2 = fig.add_subplot(132) ax3 = fig.add_subplot(133) pheatmap(ax1, mgx, &quot;Original&quot;, norm = norm) pheatmap(ax2, mgx_n0_skl, &quot;Reconstruction&quot;, norm = norm) pheatmap(ax3, mgx_n1_skl, &quot;Removed first PC&quot;, norm = norm) plt.tight_layout() plt.show() . . np.allclose(mgx_n1_svd, mgx_n1_eig) . True . Note to self: . The sklearn PCA centers the matrix before SVD internally. Hence, we have to add the mean of the rows for reconstruction. While removing PCs in symmetric matrix, it breaks the symmetry and cannot be used. .",
            "url": "https://banskt.github.io/trans-eqtl-ideas/2020/05/19/background-removal-with-PCA.html",
            "relUrl": "/2020/05/19/background-removal-with-PCA.html",
            "date": " • May 19, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Distance matrix between samples (single-KNN vs double-KNN)",
            "content": "Setup . This notebook requires: . LD filtered genotype | Gene expression matrix | Data location . #collapse-hide dosagefile = &#39;/cbscratch/sbanerj/gtex_pca/gtex_v8_filtered.dosage.raw&#39; dosage_numpy_file = &#39;/cbscratch/sbanerj/gtex_pca/gtex_dosage.npy&#39; expression_file = &#39;/scratch/sbanerj/trans-eqtl/input/gtex_v8/expression/gtex_ms_raw_std_protein_coding_lncRNA.txt&#39; . . Python libraries . #collapse-hide import numpy as np import pandas as pd from sklearn.decomposition import PCA from scipy import stats import os from scipy.cluster import hierarchy as hc import matplotlib.pyplot as plt import matplotlib from mpl_toolkits.axes_grid1 import make_axes_locatable from utils import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 300) . . Read input data . #collapse-hide def read_gtex(filename): # returns N x G gene expression expr_list = list() donor_list = list() gene_list = list() with open(filename) as mfile: donor_list = mfile.readline().strip().split(&quot; t&quot;)[1:] for line in mfile: linesplit = line.strip().split(&quot; t&quot;) gene = linesplit[0].strip() gene_list.append(gene) expr = np.array([float(x) for x in linesplit[1:]]) expr_list.append(expr) expr = np.transpose(np.array(expr_list)) return expr, donor_list, gene_list def center_expression(Y): &#39;&#39;&#39; Y is N x G here we center the columns, the mean of the columns (genes) are subtracted &#39;&#39;&#39; Ycent = (Y - np.mean(Y, axis = 0)) / np.std(Y, axis = 0) return Ycent def center_genotype(X): &#39;&#39;&#39; X is N x I here we center the columns, the mean of the columns (SNPs) are subtracted &#39;&#39;&#39; return X - np.mean(X, axis = 0).reshape(1, -1) if not os.path.isfile(dosage_numpy_file): dosage = np.loadtxt(dosagefile, delimiter=&#39; &#39;, skiprows=1, usecols=range(6, 97612)) np.save(dosage_numpy_file, dosage) else: dosage = np.load(dosage_numpy_file) gtsamples = list() with open (dosagefile, &#39;r&#39;) as infile: next(infile) for line in infile: gtsamples.append(line.strip().split()[1]) gx, gxsamples, _ = read_gtex(expression_file) gx = center_expression(gx) sampleidx = [gtsamples.index(x) for x in gxsamples] # assumes all expression samples have genotype dreduce = dosage[sampleidx, :] gt = center_genotype(dreduce) #dreduce - np.mean(dreduce, axis = 0).reshape(1, -1) print(f&#39;{len(sampleidx)} samples, {gx.shape[1]} genes, {gt.shape[1]} SNPs.&#39;) print(f&#39;Centered and normalized genotype and expression. Samples in same order as `gxsamples`&#39;) . . 706 samples, 13236 genes, 97606 SNPs. Centered and normalized genotype and expression. Samples in same order as `gxsamples` . Core function modules . Calculate PCA, distance matrix and KNN. | Map distance matrix from one sample space to another. | Remove first N principal components from any matrix X. | Plotting function. | #collapse-hide def get_pca(x, K): pca = PCA(n_components=K) pca.fit(x) # requires N x P (n_samples, n_features) x_pca = pca.transform(x) return x_pca def get_distance(a, b): return np.linalg.norm(a - b) def distance_matrix(x_pca): nsample = x_pca.shape[0] distance_matrix = np.zeros((nsample, nsample)) for i in range(nsample): for j in range(i+1, nsample): dist = get_distance(x_pca[i,:], x_pca[j,:]) distance_matrix[i, j] = dist distance_matrix[j, i] = dist return distance_matrix def map_distance_matrix(dm, samples, target_samples): N = len(target_samples) newdm = np.zeros((N, N)) newdm[:] = np.nan for i in range(N): if target_samples[i] in samples: newdm[i, i] = 0 # diagonal is always zero iold = samples.index(target_samples[i]) for j in range(i+1, N): if target_samples[j] in samples: jold = samples.index(target_samples[j]) newdm[i, j] = dm[iold, jold] newdm[j, i] = dm[jold, iold] return newdm def knn(gx, gt, dm, K, center = True): assert (gx.shape[0] == gt.shape[0]) N = gx.shape[0] gx_knn = np.zeros_like(gx) gt_knn = np.zeros_like(gt) for i in range(N): #neighbors = np.argsort(distance_matrix[i, :kneighbor + 1]) neighbors = np.argsort(dm[i, :])[:K + 1][1:] gx_knn[i, :] = gx[i, :] - np.mean(gx[neighbors, :], axis = 0) gt_knn[:, i] = gt[:, i] - np.mean(gt[:, neighbors[1:]], axis = 1) if center: gx_knn -= np.mean(gx_knn, axis = 0) gt_knn -= np.mean(gt_knn, axis = 0) return gx_knn, gt_knn def remove_nfirst_pcs(X, n=1): Xnorm = X U, S, Vt = np.linalg.svd(X, full_matrices=False) Xhat = U[:, n:] @ np.diag(S[n:]) @ Vt[n:, :] return Xhat def plot_distance_matrices(dmA, dmB, norms = None): &#39;&#39;&#39; provide norms, if required, as norms = (norm1, norm2) where, norm1 = matplotlib.colors.DivergingNorm(vmin=10., vcenter=90., vmax=170.) norm2 = matplotlib.colors.DivergingNorm(vmin=0., vcenter=90., vmax=300.) &#39;&#39;&#39; fig = plt.figure(figsize = (12, 6)) ax1 = fig.add_subplot(121) ax2 = fig.add_subplot(122) # the zero distance between the same samples # is bad for the color scale. dmA[np.diag_indices(dmA.shape[0])] = np.nan dmB[np.diag_indices(dmB.shape[0])] = np.nan cmap1 = plt.get_cmap(&quot;YlOrRd&quot;) cmap1.set_bad(&#39;w&#39;) cmap2 = plt.get_cmap(&quot;YlGnBu&quot;) cmap2.set_bad(&#39;w&#39;) if norms is not None: norm1 = norms[0] norm2 = norms[1] im1 = ax1.imshow(dmA, cmap = cmap1, norm = norm1, interpolation=&#39;nearest&#39;) im2 = ax2.imshow(dmB, cmap = cmap2, norm = norm2, interpolation=&#39;nearest&#39;) else: im1 = ax1.imshow(dmA, cmap = cmap1, interpolation=&#39;nearest&#39;) im2 = ax2.imshow(dmB, cmap = cmap2, interpolation=&#39;nearest&#39;) divider = make_axes_locatable(ax1) cax = divider.append_axes(&quot;right&quot;, size=&quot;5%&quot;, pad=0.2) cbar = plt.colorbar(im1, cax=cax, fraction = 0.1) divider = make_axes_locatable(ax2) cax = divider.append_axes(&quot;right&quot;, size=&quot;5%&quot;, pad=0.2) cbar = plt.colorbar(im2, cax=cax, fraction = 0.1) ax1.set_title(&quot;Genotype space&quot;, pad = 20) ax2.set_title(&quot;Expression space&quot;, pad = 20) plt.tight_layout() return fig . . Calculation . #collapse-show # Before KNN DT = 20 # reduced dimension of genotype DX = 30 # reduced dimension of expression dm_gt = distance_matrix(get_pca(gt, DT)) dm_gx = distance_matrix(get_pca(gx, DX)) # Expression KNN K = 30 gx_knn, gt_knn = knn(gx, gt, dm_gx, K) dm_gt_knn = distance_matrix(get_pca(gt_knn, DT)) dm_gx_knn = distance_matrix(get_pca(gx_knn, DX)) # Double KNN K1 = 10 K2 = 30 gx_knn1, gt_knn1 = knn(gx, gt, dm_gt, K1) dm_gx1 = distance_matrix(get_pca(gx_knn1, DX)) gx_knn2, gt_knn2 = knn(gx_knn1, gt_knn1, dm_gx1, K2) dm_gt_knn2 = distance_matrix(get_pca(gt_knn2, DT)) dm_gx_knn2 = distance_matrix(get_pca(gx_knn2, DX)) . . Option 1. Order samples in genotype space . #collapse-hide o1 = hc.leaves_list(hc.linkage(dm_gt, method = &#39;centroid&#39;)) . . /usr/users/sbanerj/miniconda3/envs/py36/lib/python3.7/site-packages/ipykernel_launcher.py:2: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix . Define the color bar scales . #collapse-hide norm1 = matplotlib.colors.DivergingNorm(vmin=10., vcenter=90., vmax=170.) norm2 = matplotlib.colors.DivergingNorm(vmin=0., vcenter=90., vmax=300.) norms = (norm1, norm2) . . Plot the distance matrix in genotype space and expression space . #collapse-show mgt = dm_gt[o1, :][:, o1] mgx = dm_gx[o1, :][:, o1] fig = plot_distance_matrices(mgt, mgx, norms = norms) fig.suptitle(&quot;Distance between samples before KNN&quot;) plt.show() . . #collapse-show mgt = dm_gt_knn[o1, :][:, o1] mgx = dm_gx_knn[o1, :][:, o1] fig = plot_distance_matrices(mgt, mgx, norms = norms) fig.suptitle(&quot;Distance between samples after single-KNN&quot;) plt.show() . . #collapse-show mgt = dm_gt_knn2[o1, :][:, o1] mgx = dm_gx_knn2[o1, :][:, o1] fig = plot_distance_matrices(mgt, mgx, norms = norms) fig.suptitle(&quot;Distance between samples after double-KNN&quot;) plt.show() . . Option 2. Order samples in expression space . #collapse-hide o2 = hc.leaves_list(hc.linkage(dm_gx, method=&#39;centroid&#39;)) . . /usr/users/sbanerj/miniconda3/envs/py36/lib/python3.7/site-packages/ipykernel_launcher.py:2: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix . Use the same color bar scale as above and plot the distance matrices. . #collapse-show mgt = dm_gt[o2, :][:, o2] mgx = dm_gx[o2, :][:, o2] fig = plot_distance_matrices(mgt, mgx, norms = norms) fig.suptitle(&quot;Distance between samples before KNN&quot;) plt.show() . . #collapse-show mgt = dm_gt_knn[o2, :][:, o2] mgx = dm_gx_knn[o2, :][:, o2] fig = plot_distance_matrices(mgt, mgx, norms = norms) fig.suptitle(&quot;Distance between samples after single-KNN&quot;) plt.show() . . #collapse-show mgt = dm_gt_knn2[o2, :][:, o2] mgx = dm_gx_knn2[o2, :][:, o2] fig = plot_distance_matrices(mgt, mgx, norms = norms) fig.suptitle(&quot;Distance between samples after double-KNN&quot;) plt.show() . . Difference between KNN and double-KNN . #collapse-show mgt = dm_gt_knn2[o1, :][:, o1] - dm_gt_knn[o1, :][:, o1] mgx = dm_gx_knn2[o1, :][:, o1] - dm_gx_knn[o1, :][:, o1] fig = plot_distance_matrices(mgt, mgx) plt.show() . .",
            "url": "https://banskt.github.io/trans-eqtl-ideas/2020/05/18/distance-matrix-between-samples.html",
            "relUrl": "/2020/05/18/distance-matrix-between-samples.html",
            "date": " • May 18, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Genotype principal components in GTEx",
            "content": "Setup . This notebook requires: . LD filtered genotype | Eigenvectors file from EIGENSOFT | Gene expression matrix | Data . #collapse-hide dosagefile = &#39;/cbscratch/sbanerj/gtex_pca/gtex_v8_filtered.dosage.raw&#39; dosage_numpy_file = &#39;/cbscratch/sbanerj/gtex_pca/gtex_dosage.npy&#39; eigensoft_file = &#39;/cbscratch/sbanerj/gtex_pca/GTEX_v8_2020-02-21_WGS_838Indiv_Freeze_SHAPEIT2_phased_NoMissingGT_SNPfilter_MAF0.05_allchr_ldpruned.pca.evec&#39; expression_file = &#39;/scratch/sbanerj/trans-eqtl/input/gtex_v8/expression/gtex_ms_raw_std_protein_coding_lncRNA.txt&#39; . . Python libraries . #collapse-hide import numpy as np import pandas as pd from sklearn.decomposition import PCA from scipy import stats import os import matplotlib.pyplot as plt import matplotlib from mpl_toolkits.axes_grid1 import make_axes_locatable from utils import mpl_stylesheet mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 300) . . Read input data . #collapse-hide if not os.path.isfile(dosage_numpy_file): dosage = np.loadtxt(dosagefile, delimiter=&#39; &#39;, skiprows=1, usecols=range(6, 97612)) np.save(dosage_numpy_file, dosage) else: dosage = np.load(dosage_numpy_file) gtcent = dosage - np.mean(dosage, axis = 0).reshape(1, -1) gtsamples = list() with open (dosagefile, &#39;r&#39;) as infile: next(infile) for line in infile: gtsamples.append(line.strip().split()[1]) . . Core functions . #collapse-hide def get_pca(x, K): pca = PCA(n_components=K) pca.fit(x) # requires N x P (n_samples, n_features) x_pca = pca.transform(x) return pca, x_pca def get_distance(a, b): return np.linalg.norm(a - b) def distance_matrix(x_pca): nsample = x_pca.shape[0] distance_matrix = np.zeros((nsample, nsample)) for i in range(nsample): for j in range(i+1, nsample): dist = get_distance(x_pca[i,:], x_pca[j,:]) distance_matrix[i, j] = dist distance_matrix[j, i] = dist return distance_matrix def map_distance_matrix(dm, samples, target_samples): N = len(target_samples) newdm = np.zeros((N, N)) newdm[:] = np.nan for i in range(N): if target_samples[i] in samples: newdm[i, i] = 0 # diagonal is always zero iold = samples.index(target_samples[i]) for j in range(i+1, N): if target_samples[j] in samples: jold = samples.index(target_samples[j]) newdm[i, j] = dm[iold, jold] newdm[j, i] = dm[jold, iold] return newdm . . Compare EIGENSOFT with Python PCA. . Note that PC = eigenvector * signgular value . #collapse-hide eigensoft_evec = np.loadtxt(eigensoft_file, skiprows = 0, usecols = range(1, 21)) eigensoft_sval = np.array([float(x) for x in open(eigensoft_file).readline().rstrip().split()[1:]]) pca_obj, gtpca = get_pca(gtcent, 20) . . mpl_stylesheet.banskt_presentation(fontfamily = &#39;latex-clearsans&#39;, fontsize = 18, colors = &#39;banskt&#39;, dpi = 300) fig = plt.figure(figsize = (18, 6)) ax1 = fig.add_subplot(131) ax2 = fig.add_subplot(132) ax3 = fig.add_subplot(133) ax1.scatter(gtpca[:, 0], gtpca[:, 1], s = 20, marker = &#39;o&#39;, edgecolor = &#39;black&#39;, color = &#39;gray&#39;, alpha = 0.2) ax1.set_xlabel(&quot;PC1&quot;) ax1.set_ylabel(&quot;PC2&quot;) ax1.set_title(&quot;Python Sklearn&quot;, pad = 20) ax2.scatter(eigensoft_evec[:, 0], eigensoft_evec[:, 1], s = 20, marker = &#39;o&#39;, edgecolor = &#39;black&#39;, color = &#39;gray&#39;, alpha = 0.2) ax2.set_xlabel(&quot;eigenvector 1&quot;) ax2.set_ylabel(&quot;eigenvector 2&quot;) ax2.set_title(&quot;EIGENSOFT&quot;, pad = 20) k = 1 ax3.scatter(gtpca[:, k], eigensoft_evec[:, k] * pca_obj.singular_values_[k], s = 20, marker = &#39;o&#39;, edgecolor = &#39;black&#39;, color = &#39;gray&#39;, alpha = 0.2) ax3.set_xlabel(&quot;Python Sklearn&quot;) ax3.set_ylabel(&quot;EIGENSOFT&quot;) ax3.set_title(f&#39;Principal Component {k}&#39;, pad = 20) plt.tight_layout() plt.show() . Reducing genotype dimension . I need to reduce the dimensionality of the genotype matrix for calculating distance matrix between samples in the reduced dimension of the genotype space. It might be interesting to check how the target dimension $D$ affects the distance matrix. Here, we checked $D=20$ and $D=40$. . #collapse-hide pcagt20_obj, gtpca_20 = get_pca(gtcent, 20) pcagt40_obj, gtpca_40 = get_pca(gtcent, 40) dm20 = distance_matrix(gtpca_20) dm40 = distance_matrix(gtpca_40) . . I calculated the hierarchical clusters from the distance matrix calculated with $D=20$. . #collapse-hide from scipy.cluster import hierarchy as hc link = hc.linkage(dm20, method=&#39;centroid&#39;) o1 = hc.leaves_list(link) . . /usr/users/sbanerj/miniconda3/envs/py36/lib/python3.7/site-packages/ipykernel_launcher.py:3: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix This is separate from the ipykernel package so we can avoid doing imports until . and ordered the samples in the distance matrix accordingly. . #collapse-hide mgt20 = dm20[o1,:][:,o1] mgt40 = dm40[o1,:][:,o1] . . Here, we look at the difference between the distance matrices calculated with $D=20$ and $D=40$. . #collapse-show fig = plt.figure(figsize = (18, 6)) ax1 = fig.add_subplot(131) ax2 = fig.add_subplot(132) ax3 = fig.add_subplot(133) norm = matplotlib.colors.DivergingNorm(vmin=0, vcenter=50., vmax=170.) cmap = plt.get_cmap(&quot;YlOrRd&quot;) im1 = ax1.imshow(mgt20, cmap=cmap, norm = norm, interpolation=&#39;nearest&#39;) im2 = ax2.imshow(mgt40, cmap=cmap, norm = norm, interpolation=&#39;nearest&#39;) im3 = ax3.imshow(np.abs(mgt20 - mgt40), cmap = cmap, norm = norm, interpolation=&#39;nearest&#39;) divider = make_axes_locatable(ax1) cax = divider.append_axes(&quot;right&quot;, size=&quot;5%&quot;, pad=0.2) cbar = plt.colorbar(im1, cax=cax, fraction = 0.1) divider = make_axes_locatable(ax2) cax = divider.append_axes(&quot;right&quot;, size=&quot;5%&quot;, pad=0.2) cbar = plt.colorbar(im2, cax=cax, fraction = 0.1) divider = make_axes_locatable(ax3) cax = divider.append_axes(&quot;right&quot;, size=&quot;5%&quot;, pad=0.2) cbar = plt.colorbar(im3, cax=cax, fraction = 0.1) ax1.set_title(&quot;D = 20&quot;, pad = 20) ax2.set_title(&quot;D = 40&quot;, pad = 20) ax3.set_title(&quot;Difference&quot;, pad = 20) plt.tight_layout() #plt.savefig(&#39;../plots/gtex_samples_in_genotype_space.png&#39;, bbox_inches=&#39;tight&#39;) plt.show() . . Variance explained by the eigenvectors . #collapse-hide fig = plt.figure(figsize = (12, 6)) ax1 = fig.add_subplot(111) ax1.bar(np.arange(1, 21), eigensoft_sval) ax1.set_xlabel(&#39;Number of components&#39;) ax1.set_ylabel(&#39;Eigenvalues&#39;) ax1.set_xticks(list(range(0, 21, 4))) plt.tight_layout() #plt.savefig(&#39;../plots/gtex_genotype_pc_explained_variance.png&#39;, bbox_inches=&#39;tight&#39;) plt.show() . . Relationship with sample distance in expression spaces . Read the GTEx gene expression . #collapse-hide def read_gtex(filename): # returns N x G gene expression expr_list = list() donor_list = list() gene_list = list() with open(filename) as mfile: donor_list = mfile.readline().strip().split(&quot; t&quot;)[1:] for line in mfile: linesplit = line.strip().split(&quot; t&quot;) gene = linesplit[0].strip() gene_list.append(gene) expr = np.array([float(x) for x in linesplit[1:]]) expr_list.append(expr) expr = np.transpose(np.array(expr_list)) return expr, donor_list, gene_list . . gx, gxsamples, genelist = read_gtex(expression_file) . and reduce the dimension. . For the expression matrix, I reduced dimension $D&#39; = N, 20, 40$ where $N$ is the number of samples . _, gxpca_n = get_pca(gx, gx.shape[0]) _, gxpca_20 = get_pca(gx, 20) _, gxpca_40 = get_pca(gx, 40) . dmgxN = distance_matrix(gxpca_n) dmgx20 = distance_matrix(gxpca_20) dmgx40 = distance_matrix(gxpca_40) . target_samples = [gtsamples[x] for x in o1] mgxN = map_distance_matrix(dmgxN, gxsamples, target_samples) mgx20 = map_distance_matrix(dmgx20, gxsamples, target_samples) mgx40 = map_distance_matrix(dmgx40, gxsamples, target_samples) . fig = plt.figure(figsize = (12, 12)) ax1 = fig.add_subplot(221) ax2 = fig.add_subplot(222) ax3 = fig.add_subplot(223) ax4 = fig.add_subplot(224) cmap1 = plt.get_cmap(&quot;YlOrRd&quot;) cmap2 = plt.get_cmap(&quot;GnBu&quot;) #cmap2.set_bad(&#39;paleturquoise&#39;) cmap2.set_bad(&#39;w&#39;) im1 = ax1.imshow(mgt20, cmap = cmap1, interpolation=&#39;nearest&#39;) im2 = ax2.imshow(mgxN, cmap = cmap2, interpolation=&#39;nearest&#39;) im3 = ax3.imshow(mgx20, cmap = cmap2, interpolation=&#39;nearest&#39;) im4 = ax4.imshow(mgx40, cmap = cmap2, interpolation=&#39;nearest&#39;) for axi, imi in zip([ax1, ax2, ax3, ax4], [im1, im2, im3, im4]): divider = make_axes_locatable(axi) cax = divider.append_axes(&quot;right&quot;, size=&quot;5%&quot;, pad=0.2) cbar = plt.colorbar(imi, cax=cax, fraction = 0.1) ax1.set_title(&quot;Genotype space&quot;, pad = 20) for axi, dim in zip([ax2, ax3, ax4], [&#39;N&#39;, &#39;20&#39;, &#39;40&#39;]): axi.set_title(f&quot;Expression space, $D = {dim}$&quot;, pad = 20) plt.tight_layout() #plt.savefig(&#39;../plots/gtex_samples_in_expression_space.png&#39;, bbox_inches=&#39;tight&#39;) plt.show() .",
            "url": "https://banskt.github.io/trans-eqtl-ideas/2020/05/16/genotype-PCA-double-KNN.html",
            "relUrl": "/2020/05/16/genotype-PCA-double-KNN.html",
            "date": " • May 16, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://banskt.github.io/trans-eqtl-ideas/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://banskt.github.io/trans-eqtl-ideas/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}